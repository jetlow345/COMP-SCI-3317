{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CIgMgrNf85n"
   },
   "source": [
    "# Transfer learning & fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQNvW8oBf85q"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRnY5ZzUf85r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYiHLgXSf85t"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Transfer learning** consists of taking features learned on one problem, and\n",
    "leveraging them on a new, similar problem. For instance, features from a model that has\n",
    "learned to identify racoons may be useful to kick-start a model meant to identify\n",
    " tanukis.\n",
    "\n",
    "Transfer learning is usually done for tasks where your dataset has too little data to\n",
    " train a full-scale model from scratch.\n",
    "\n",
    "The most common incarnation of transfer learning in the context of deep learning is the\n",
    " following workflow:\n",
    "\n",
    "1. Take layers from a previously trained model.\n",
    "2. Freeze them, so as to avoid destroying any of the information they contain during\n",
    " future training rounds.\n",
    "3. Add some new, trainable layers on top of the frozen layers. They will learn to turn\n",
    " the old features into predictions on a  new dataset.\n",
    "4. Train the new layers on your dataset.\n",
    "\n",
    "A last, optional step, is **fine-tuning**, which consists of unfreezing the entire\n",
    "model you obtained above (or part of it), and re-training it on the new data with a\n",
    "very low learning rate. This can potentially achieve meaningful improvements, by\n",
    " incrementally adapting the pretrained features to the new data.\n",
    "\n",
    "First, we will go over the Keras `trainable` API in detail, which underlies most\n",
    " transfer learning & fine-tuning workflows.\n",
    "\n",
    "Then, we'll demonstrate the typical workflow by taking a model pretrained on the\n",
    "ImageNet dataset, and retraining it on the Kaggle \"cats vs dogs\" classification\n",
    " dataset.\n",
    "\n",
    "This is adapted from\n",
    "[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n",
    " and the 2016 blog post\n",
    "[\"building powerful image classification models using very little\n",
    " data\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i14H60dEf85u"
   },
   "source": [
    "## Freezing layers: understanding the `trainable` attribute\n",
    "\n",
    "Layers & models have three weight attributes:\n",
    "\n",
    "- `weights` is the list of all weights variables of the layer.\n",
    "- `trainable_weights` is the list of those that are meant to be updated (via gradient\n",
    " descent) to minimize the loss during training.\n",
    "- `non_trainable_weights` is the list of those that aren't meant to be trained.\n",
    " Typically they are updated by the model during the forward pass.\n",
    "\n",
    "**Example: the `Dense` layer has 2 trainable weights (kernel & bias)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcQkbabjf85v",
    "outputId": "c3d8e287-bf98-4cf9-b5f4-00fc70be2a4a"
   },
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4))  # Create a input of 4 features\n",
    "\n",
    "print(\"weights:\", len(layer.weights)) # You have two weights (weights and biases)\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can check the weights available in a dense layer\n",
    "layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYlE5Oj_f85w"
   },
   "source": [
    "In general, all weights are trainable weights. The only built-in layer that has\n",
    "non-trainable weights is the `BatchNormalization` layer. It uses non-trainable weights\n",
    " to keep track of the mean and variance of its inputs during training.\n",
    "To learn how to use non-trainable weights in your own custom layers, see the\n",
    "[guide to writing new layers from scratch](https://keras.io/guides/making_new_layers_and_models_via_subclassing/).\n",
    "\n",
    "**Example: the `BatchNormalization` layer has 2 trainable weights and 2 non-trainable\n",
    " weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIJ1iZJ1f85w"
   },
   "outputs": [],
   "source": [
    "# We try the same with BatchNormalization\n",
    "layer = keras.layers.BatchNormalization()\n",
    "layer.build((None, 4))  # Create a input of 4 features\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the weights available in a dense layer\n",
    "layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, gamma y beta are learned variables\n",
    "layer.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving mean and moving variance are non-trainable variables\n",
    "layer.non_trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmaB3gv4f85x"
   },
   "source": [
    "Layers & models also feature a boolean attribute `trainable`. Its value can be changed.\n",
    "Setting `layer.trainable` to `False` moves all the layer's weights from trainable to\n",
    "non-trainable.  This is called \"freezing\" the layer: the state of a frozen layer won't\n",
    "be updated during training (either when training with `fit()` or when training with\n",
    " any custom loop that relies on `trainable_weights` to apply gradient updates).\n",
    "\n",
    "**Example: setting `trainable` to `False`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUE9Xc31f85y"
   },
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4))  # Create a input of 4 features\n",
    "layer.trainable = False  # Freeze the layer\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-KnudqFf85z"
   },
   "source": [
    "When a trainable weight becomes non-trainable, its value is no longer updated during\n",
    " training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-J7W9t2Of85z"
   },
   "outputs": [],
   "source": [
    "# Define two layers with three neurons each one\n",
    "layer1 = keras.layers.Dense(3, activation=\"relu\")\n",
    "layer2 = keras.layers.Dense(2, activation=\"sigmoid\")\n",
    "# Create the model \n",
    "model = keras.Sequential([keras.Input(shape=(3,)), layer1, layer2])\n",
    "\n",
    "# Freeze the first layer\n",
    "layer1.trainable = False\n",
    "\n",
    "# Keep a copy of the weights of layer1 for later reference\n",
    "initial_layer1_weights_values = layer1.get_weights()\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Create random X and y values\n",
    "X = np.random.random((2, 3)) # Two samples, three features\n",
    "y = np.random.random((2, 2)) # Two samples, two features\n",
    "\n",
    "# Check the data\n",
    "print(\"X: \", X)\n",
    "print(\"y: \", y)\n",
    "\n",
    "# Train using random values in X and y. 1 epoch\n",
    "model.fit(X, y)\n",
    "\n",
    "# Check that the weights of layer1 have not changed during training\n",
    "final_layer1_weights_values = layer1.get_weights()\n",
    "\n",
    "# Consider that get_weights for a dense layer returns a matrix of weights and a vector of biases\n",
    "\n",
    "# We can check the weights in first layer before training\n",
    "print(\"\\ninitial layer1 weights values: \\n\",initial_layer1_weights_values)\n",
    "# Now we can check the weights in first layer after training\n",
    "print(\"\\nfinal layer1 weights values: \\n\",final_layer1_weights_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD5CRbXnf851"
   },
   "source": [
    "## Recursive setting of the `trainable` attribute\n",
    "\n",
    "If you set `trainable = False` on a model or on any layer that has sublayers,\n",
    "all children layers become non-trainable as well.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bj-aB2o5f852"
   },
   "outputs": [],
   "source": [
    "# e go to define a simple model\n",
    "inner_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(3,)),\n",
    "        keras.layers.Dense(3, activation=\"relu\"),\n",
    "        keras.layers.Dense(3, activation=\"relu\"),\n",
    "    ])\n",
    "\n",
    "# We go to define another simple model, which includes the previous one\n",
    "model = keras.Sequential(\n",
    "    [keras.Input(shape=(3,)), inner_model, keras.layers.Dense(3, activation=\"sigmoid\")])\n",
    "\n",
    "# We can set all weights non-trainable\n",
    "model.trainable = False  # Freeze the outer model\n",
    "\n",
    "# We can cehck both the entire model and individual inner layers are frozen\n",
    "print(\"Weights in our model are trainable?: \", inner_model.trainable) \n",
    "print(\"Weights in our first layer are trainable?: \",inner_model.layers[0].trainable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4LWblY4f853"
   },
   "source": [
    "## The typical transfer-learning workflow\n",
    "\n",
    "This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
    "\n",
    "1. Instantiate a base model and load pre-trained weights into it.\n",
    "2. Freeze all layers in the base model by setting `trainable = False`.\n",
    "3. Create a new model on top of the output of one (or several) layers from the base\n",
    " model.\n",
    "4. Train your new model on your new dataset.\n",
    "\n",
    "Note that an alternative, more lightweight workflow could also be:\n",
    "\n",
    "1. Instantiate a base model and load pre-trained weights into it.\n",
    "2. Run your new dataset through it and record the output of one (or several) layers\n",
    " from the base model. This is called **feature extraction**.\n",
    "3. Use that output as input data for a new, smaller model.\n",
    "\n",
    "A key advantage of that second workflow is that you only run the base model once on\n",
    " your data, rather than once per epoch of training. So it's a lot faster & cheaper.\n",
    "\n",
    "An issue with that second workflow, though, is that it doesn't allow you to dynamically\n",
    "modify the input data of your new model during training, which is required when doing\n",
    "data augmentation, for instance. Transfer learning is typically used for tasks when\n",
    "your new dataset has too little data to train a full-scale model from scratch, and in\n",
    "such scenarios data augmentation is very important. So in what follows, we will focus\n",
    " on the first workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load a pretrained model\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.    \n",
    "    input_shape=(120, 120, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top (final layer)\n",
    "# You add the parameter include_top = False to not include the final layer of the base model\n",
    "\n",
    "# Then, freeze the base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see the architecture of this model\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and add custom top layers\n",
    "model = keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.GlobalAveragePooling2D()) # Technique to reduce the data. \n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.utils import img_to_array, array_to_img\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train[0:1000]\n",
    "X_test = X_test[0:100]\n",
    "\n",
    "# We are working with 1 channel images, but this model needs 3 channels. e repeat the data 3 times\n",
    "X_train_3c = np.dstack([X_train] * 3)\n",
    "X_test_3c = np.dstack([X_test]*3)\n",
    "\n",
    "# We create a 3d image. Each channel contains same data\n",
    "X_train_3d = X_train_3c.reshape((X_train_3c.shape[0], 28, 28, 3)) \n",
    "X_test_3d = X_test_3c.reshape((X_test_3c.shape[0], 28, 28, 3))\n",
    "\n",
    "# Change the dimmension of data\n",
    "X_train_resized = np.array([img_to_array(array_to_img(im, scale=False).resize((120, 120))) for im in X_train_3d]) / 255.0\n",
    "X_test_resized = np.array([img_to_array(array_to_img(im, scale=False).resize((120, 120))) for im in X_test_3d]) / 255.0\n",
    "\n",
    "# Create classes\n",
    "y_train = to_categorical(y_train[0:1000], num_classes=10)\n",
    "y_test = to_categorical(y_test[0:100], num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_resized, y_train, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUB9k0ULf853"
   },
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Once your model has converged on the new data, you can try to unfreeze **all or part of\n",
    " the base model** and retrain the whole model end-to-end with a very low learning rate.\n",
    "\n",
    "This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.\n",
    "\n",
    "It is critical to only do this step *after* the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.\n",
    "\n",
    "It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of layers to unfreeze\n",
    "num_layers_to_unfreeze = 3\n",
    "\n",
    "# Unfreeze the top layers\n",
    "for layer in base_model.layers[-num_layers_to_unfreeze:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.000001),  # Very low learning rate\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "model.fit(X_train_resized, y_train, epochs=1) # We are using 1 epoch only because it takes to much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4Mztdj8f854"
   },
   "source": [
    "## An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset. \n",
    "\n",
    "**Try to run this in your free time since it takes time**\n",
    "\n",
    "To solidify these concepts, let's walk you through a concrete end-to-end transfer\n",
    "learning & fine-tuning example. We will load the Xception model, pre-trained on\n",
    " ImageNet, and use it on the Kaggle \"cats vs. dogs\" classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLISkfLbf855"
   },
   "source": [
    "### Getting the data\n",
    "\n",
    "First, let's fetch the cats vs. dogs dataset using TFDS. If you have your own dataset,\n",
    "you'll probably want to use the utility\n",
    "`tf.keras.utils.image_dataset_from_directory` to generate similar labeled\n",
    " dataset objects from a set of images on disk filed into class-specific folders.\n",
    "\n",
    "Transfer learning is most useful when working with very small datasets. To keep our\n",
    "dataset small, we will use 40% of the original training data (25,000 images) for\n",
    " training, 10% for validation, and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorflow_datasets\n",
    "#!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAPZF6cqf855"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar() # To disable an annoying progress bar\n",
    "train_ds, validation_ds, test_ds = tfds.load(           \n",
    "    \"cats_vs_dogs\",\n",
    "    # Reserve 10% for validation and 10% for test\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
    "    as_supervised=True)  # Include labels\n",
    "\n",
    "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
    "print(\"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds))\n",
    "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSUjiZ36f855"
   },
   "source": [
    "These are the first 9 images in the training dataset -- as you can see, they're all\n",
    " different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n_N6UoUf856"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(int(label))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkq_hTKJf856"
   },
   "source": [
    "We can also see that label 1 is \"dog\" and label 0 is \"cat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_vq-_LMf856"
   },
   "source": [
    "### Standardizing the data\n",
    "\n",
    "Our raw images have a variety of sizes. In addition, each pixel consists of 3 integer\n",
    "values between 0 and 255 (RGB level values).\n",
    "\n",
    "In general, it's a good practice to develop models that take raw data as input, as\n",
    "opposed to models that take already-preprocessed data. The reason being that, if your\n",
    "model expects preprocessed data, any time you export your model to use it elsewhere\n",
    "(in a web browser, in a mobile app), you'll need to reimplement the exact same\n",
    "preprocessing pipeline. This gets very tricky very quickly. So we should do the least\n",
    " possible amount of preprocessing before hitting the model.\n",
    "\n",
    "Here, we'll do image resizing in the data pipeline (because a deep neural network can\n",
    "only process contiguous batches of data), and we'll do the input value scaling as part\n",
    " of the model, when we create it.\n",
    "\n",
    "Let's resize images to 120x120:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4SikYGWf857"
   },
   "outputs": [],
   "source": [
    "size = (120, 120)\n",
    "# This is a way how to reshape images\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, let's batch the data and use caching & prefetching to optimize loading speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# This is a way how to save images in cache in order to improve speed\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g6-XuBof858"
   },
   "source": [
    "### Using random data augmentation\n",
    "\n",
    "When you don't have a large image dataset, it's a good practice to artificially\n",
    " introduce sample diversity by applying random yet realistic transformations to\n",
    "the training images, such as random horizontal flipping or small random rotations. This\n",
    "helps expose the model to different aspects of the training data while slowing down\n",
    " overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Brecd4Jyf858"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# This is a process useful to broaden the dataset. Here we include a rotation technique\n",
    "data_augmentation = keras.Sequential(\n",
    "    [keras.layers.RandomFlip(\"horizontal\"), keras.layers.RandomRotation(0.1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_UR36wpf859"
   },
   "source": [
    "Let's visualize what the first image of the first batch looks like after various random\n",
    " transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq6fhuUef859"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = images[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(\n",
    "            tf.expand_dims(first_image, 0), training=True\n",
    "        )\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
    "        plt.title(int(labels[0]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI9w_cFnf85-"
   },
   "source": [
    "## Build a model\n",
    "\n",
    "Now let's built a model that follows the blueprint we've explained earlier.\n",
    "\n",
    "Note that:\n",
    "\n",
    "- We add a `Rescaling` layer to scale input values (initially in the `[0, 255]`\n",
    " range) to the `[-1, 1]` range. This is because Xception require data in this way\n",
    "- We add a `Dropout` layer before the classification layer, for regularization.\n",
    "- We make sure to pass `training=False` when calling the base model, so that\n",
    "it runs in inference mode, so that batchnorm statistics don't get updated\n",
    "even after we unfreeze the base model for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QjpU-Irf85-"
   },
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(120, 120, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top. \n",
    "\n",
    "# This is other way how to create a model.\n",
    "inputs = keras.Input(shape=(120, 120, 3))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be scaled\n",
    "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
    "# outputs: `(inputs * scale) + offset`. Values are given in Keras page: https://keras.io/api/layers/preprocessing_layers/image_preprocessing/rescaling/\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA7cZnINf85_"
   },
   "source": [
    "## Train the top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MHK6h-Tf85_"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6316pc1Uf85_"
   },
   "source": [
    "## Do a round of fine-tuning of the entire model\n",
    "\n",
    "Finally, let's unfreeze the base model and train the entire model end-to-end with a low\n",
    " learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkNdxcs9f85_"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. \n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "epochs = 10\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4uUgy9Mf86A"
   },
   "source": [
    "After 10 epochs, fine-tuning gains us a nice improvement here."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "transfer_learning",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
